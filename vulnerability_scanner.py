from contextlib import contextmanager
from datetime import datetime
import json
import time
import traceback
import langchain
import langchain.globals
from langchain.output_parsers import PydanticOutputParser
from langchain.prompts.chat import ChatPromptTemplate
from langchain.prompts.chat import HumanMessagePromptTemplate
from langchain_groq import ChatGroq
from langchain.schema import HumanMessage
from typing import Generator, List, Dict, Optional, Any
from pydantic import BaseModel, Field
from enum import Enum
import logging
from pathlib import Path, WindowsPath
import concurrent.futures
from rich.console import Console
from rich.table import Table
from jinja2 import Template
import os
from reportlab.lib import colors
from reportlab.lib.pagesizes import letter
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.pdfgen import canvas
from reportlab.platypus import (
    SimpleDocTemplate,
    Paragraph,
    Spacer,
    Table,
    TableStyle,
    PageBreak,
    Flowable,
    Frame,
)
from .helper import (
    HTML_TEMPLATE,
    VulnerabilityType,
    VulnerabilityFinding,
    VulnerabilityList,
    SecurityScanResult,
    ChartDrawing,
)

logger = logging.getLogger(__name__)
langchain.globals.set_verbose(False)
console = Console()

class SecurityScannerError(Exception):
    """
    Custom exception class for SecurityScanner errors.

    This class is used to raise specific exceptions related to the
    SecurityScanner during operations such as file scanning or analysis.
    """

    pass


class SecurityScanner:
    """Security scanner using LangChain for structured output"""

    def __init__(self, code_assistant):
        """Initialize scanner with CodeAssistant instance"""
        self.code_assistant = code_assistant
        self.setup_parser_and_prompt()
        #  Configure logging with more detailed format
        logging.basicConfig(
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            level=logging.INFO,
        )
        self.logger = logging.getLogger(__name__)
        self.scan_duration = 0

    @contextmanager
    def _timer(self) -> Generator[None, None, None]:
        """
        Context manager to measure execution time.

        Yields:
            None: Allows the surrounding code block to execute, while measuring
            the time taken for execution.
        """
        start = time.time()
        try:
            yield
        finally:
            self.scan_duration = time.time() - start

    def setup_parser_and_prompt(self):
        """
        Setup LangChain parser and prompt template.

        Initializes a Pydantic parser for structured output and a ChatPromptTemplate
        to format input prompts for vulnerability analysis.
        """
        # Initialize the Pydantic parser
        self.parser = PydanticOutputParser(pydantic_object=VulnerabilityList)

        # Create the prompt template with format instructions
        template = """VULNERABILITY ANALYSIS INSTRUCTIONS - STRICT COMPLIANCE REQUIRED

        ROLE: You are a senior cybersecurity expert specializing in static code analysis 
        with over 20 years of experience in identifying security vulnerabilities.
        Analyze the following code and identify all security vulnerabilities according to OWASP Top 10 2021.

        For each vulnerability found, you must provide:
        1. The exact OWASP category
        2. File path (if available)
        3. The line number where it occurs
        4. The vulnerable code snippet
        5. A detailed technical description
        6. The severity level
        7. Specific, actionable recommendations
        8. Corrected version of the vulnerable code
        9. The CWE ID (if applicable)
        10. Risk score (0-10)
        11. Detailed impact analysis

        MANDATORY OUTPUT FORMAT CONSTRAINTS:
        - MUST be valid JSON-serializable
        - MUST match VulnerabilityList Pydantic model
        - MUST include ALL specified fields
        - NO PLACEHOLDER OR INCOMPLETE ENTRIES

        Code to analyze:
        {code}

        Additional context (if any):
        {context}

        OWASP TOP 10 VULNERABILITY CATEGORIES (MANDATORY REFERENCE):
        1. A01:2021-Injection
        2. A02:2021-Cryptographic Failures
        3. A03:2021-Broken Authentication
        4. A04:2021-Insecure Design
        5. A05:2021-Security Misconfiguration
        6. A06:2021-Vulnerable Components
        7. A07:2021-Auth Failure
        8. A08:2021-Data Integrity Failures
        9. A09:2021-Security Logging Failures
        10. A10:2021-SSRF

        CRITICAL INSTRUCTION: 
        - If NO vulnerabilities found, return an EMPTY vulnerabilities list
        - NEVER fabricate vulnerabilities
        - Be EXHAUSTIVE but PRECISE

        Provide your output **strictly** in the following JSON format without adding any extra text, explanations, or commentary.
        Do not prepend or append any content other than the JSON object.
        {format_instructions}
        """

        self.prompt = ChatPromptTemplate.from_messages(
            [HumanMessagePromptTemplate.from_template(template)]
        )

    def scan_files(
        self,
        directory_path: str,
        file_pattern: str = "*.py",
        max_workers: Optional[int] = None,
    ) -> SecurityScanResult:
        """
        Scan multiple files in a directory for vulnerabilities.

        Args:
            directory_path (str): Path to the directory containing files to scan.
            file_pattern (str): Glob pattern to filter files (default: "*.py").
            max_workers (Optional[int]): Maximum number of threads to use.

        Returns:
            SecurityScanResult: Aggregated vulnerabilities, metrics, and summary
            of the scan process.

        Raises:
            SecurityScannerError: If the directory does not exist or a critical
            error occurs during scanning.
        """
        try:
            with self._timer():
                directory = Path(directory_path)
                if not directory.exists():
                    raise SecurityScannerError(
                        f"Directory {directory_path} does not exist"
                    )

                files = list(directory.rglob(file_pattern))
                if not files:
                    self.logger.warning(
                        f"No files matching pattern {file_pattern} found in {directory_path}"
                    )
                    return SecurityScanResult(
                        vulnerabilities=[], metrics={}, scan_summary={}
                    )

                all_vulnerabilities = []
                scan_metrics = {
                    "files_scanned": len(files),
                    "total_lines_scanned": 0,
                    "scan_duration": 0,
                    "vulnerabilities_per_file": {},
                    "failed_scans": [],
                }

                # Determine optimal number of workers based on CPU cores
                max_workers = max_workers or min(8, (os.cpu_count() or 1) * 2)

                with concurrent.futures.ThreadPoolExecutor(
                    max_workers=max_workers
                ) as executor:
                    future_to_file = {
                        executor.submit(self._scan_single_file, file_path): file_path
                        for file_path in files
                    }

                    for future in concurrent.futures.as_completed(future_to_file):
                        file_path = future_to_file[future]
                        try:
                            result = future.result()
                            all_vulnerabilities.extend(result.vulnerabilities)
                            scan_metrics["vulnerabilities_per_file"][str(file_path)] = (
                                len(result.vulnerabilities)
                            )
                            scan_metrics["total_lines_scanned"] += result.metrics[
                                "lines_of_code"
                            ]
                        except Exception as e:
                            self.logger.error(
                                f"Error scanning {file_path}: {str(e)}\n{traceback.format_exc()}"
                            )
                            scan_metrics["failed_scans"].append(
                                {"file": str(file_path), "error": str(e)}
                            )

                scan_metrics["scan_duration"] = self.scan_duration
                scan_summary = self._generate_scan_summary(
                    all_vulnerabilities, scan_metrics
                )

                return SecurityScanResult(
                    vulnerabilities=all_vulnerabilities,
                    metrics=scan_metrics,
                    scan_summary=scan_summary,
                )

        except Exception as e:
            error_msg = (
                f"Critical error in directory scan: {str(e)}\n{traceback.format_exc()}"
            )
            self.logger.error(error_msg)
            raise SecurityScannerError(error_msg) from e

    def _scan_single_file(self, file_path: str) -> SecurityScanResult:
        """
        Scan a single file for vulnerabilities.

        Args:
            file_path (str): Path to the file to be scanned.

        Returns:
            SecurityScanResult: Detected vulnerabilities and scanning metrics.

        Raises:
            Exception: If the file cannot be read or an error occurs during analysis.
        """
        try:
            with open(file_path, "r") as f:
                code = f.read()

            vulnerabilities = self._llm_analysis(code, {"file_path": file_path})

            metrics = {
                "lines_of_code": len(
                    [line for line in code.splitlines() if line.strip()]
                ),
                "file_path": file_path,
            }

            return SecurityScanResult(
                vulnerabilities=vulnerabilities, metrics=metrics, scan_summary={}
            )
        except Exception as e:
            logger.error(f"Error scanning file {file_path}: {str(e)}")
            raise

    def scan_code(self, code: str) -> SecurityScanResult:
        """
        Scan a code snippet for vulnerabilities using LLM-based analysis.

        Args:
            code (str): The source code to analyze.

        Returns:
            SecurityScanResult: Detected vulnerabilities, metrics, and a scan summary.

        Raises:
            Exception: If an error occurs during the code scan process.
        """
        try:
            all_vulnerabilities = []
            scan_metrics = {
                "files_scanned": 1,
                "total_lines_scanned": 0,
                "scan_duration": 0,
                "vulnerabilities_per_file": {},
            }

            vulnerabilities = self._llm_analysis(code)
            metrics = {
                "lines_of_code": len(
                    [line for line in code.splitlines() if line.strip()]
                )
            }
            result = SecurityScanResult(
                vulnerabilities=vulnerabilities, metrics=metrics, scan_summary={}
            )

            try:
                all_vulnerabilities.extend(result.vulnerabilities)
                scan_metrics["total_lines_scanned"] += result.metrics["lines_of_code"]
            except Exception as e:
                logger.error(f"Error scanning code: {str(e)}")

            scan_summary = self._generate_scan_summary(
                all_vulnerabilities, scan_metrics
            )
            return SecurityScanResult(
                vulnerabilities=all_vulnerabilities,
                metrics=scan_metrics,
                scan_summary=scan_summary,
            )
        except Exception as e:
            logger.error(f"Error scanning code: {str(e)}")
            return []

    def _llm_analysis(
        self, code: str, context: Optional[Dict[str, Any]] = None
    ) -> List[VulnerabilityFinding]:
        """
        Perform LLM-based security analysis with structured output.

        Args:
            code (str): The source code to analyze.
            context (Optional[Dict[str, Any]]): Additional context (e.g., file path).

        Returns:
            List[VulnerabilityFinding]: List of vulnerabilities identified by the LLM.

        Raises:
            Exception: If the LLM fails or an error occurs during analysis.
        """
        try:
            # Format the prompt with parser instructions and code
            context = context or {}
            prompt_with_instructions = self.prompt.format_prompt(
                format_instructions=self.parser.get_format_instructions(),
                code=code,
                context=str(context),
            ).to_string()

            # Get LLM response
            if "gpt4" in self.code_assistant.llms:
                llm = self.code_assistant.llms["gpt4"]
                response = llm.complete(prompt_with_instructions)
                response_text = response.text
            else:
                llm = ChatGroq(
                    api_key=self.code_assistant.groq_api_key,
                    model_name="llama-3.1-8b-instant",
                    temperature=0.1,
                    max_tokens=2000,
                )
                response = llm.invoke([HumanMessage(content=prompt_with_instructions)])
                response_text = response.content
                response.pretty_print()

            # Parse the response using LangChain parser
            parsed_output = self.parser.parse(response_text)

            # Add file path to vulnerabilities if provided in context
            if "file_path" in context:
                for vuln in parsed_output.vulnerabilities:
                    vuln.file_path = context["file_path"]

            return parsed_output.vulnerabilities

        except Exception as e:
            logger.error(f"Error in LLM analysis: {str(e)}")
            return []

    def _generate_scan_summary(
        self, vulnerabilities: List[VulnerabilityFinding], metrics: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate a comprehensive scan summary from the results.

        Args:
            vulnerabilities (List[VulnerabilityFinding]): List of detected vulnerabilities.
            metrics (Dict[str, Any]): Scan metrics including line counts and timing.

        Returns:
            Dict[str, Any]: A dictionary summarizing the vulnerabilities, severity
            distribution, risk scores, and affected files.
        """
        summary = {
            "total_vulnerabilities": len(vulnerabilities),
            "severity_distribution": {"High": 0, "Medium": 0, "Low": 0},
            "vulnerability_types": {},
            "average_risk_score": 0,
            "highest_risk_vulnerabilities": [],
            "files_affected": len(metrics["vulnerabilities_per_file"]),
            "total_lines_scanned": metrics["total_lines_scanned"],
        }

        risk_scores = []
        for vuln in vulnerabilities:
            summary["severity_distribution"][vuln.severity] += 1

            if vuln.vulnerability_type not in summary["vulnerability_types"]:
                summary["vulnerability_types"][vuln.vulnerability_type] = 0
            summary["vulnerability_types"][vuln.vulnerability_type] += 1

            risk_scores.append(vuln.risk_score)

            if vuln.risk_score >= 8.0:
                summary["highest_risk_vulnerabilities"].append(
                    {
                        "type": vuln.vulnerability_type,
                        "risk_score": vuln.risk_score,
                        "file": vuln.file_path,
                        "description": vuln.description,
                    }
                )

        if risk_scores:
            summary["average_risk_score"] = sum(risk_scores) / len(risk_scores)

        return summary

    def generate_report(
        self,
        scan_result: SecurityScanResult,
        output_format: str = "console",
        output_path: Optional[str] = None,
    ) -> str:
        """Generate formatted security report in specified output format.

        Args:
            scan_result: The security scan results to report
            output_format: Format of the report ('console', 'html', 'pdf', or 'json')
            output_path: Optional custom path for the report file

        Returns:
            str: Path to the generated report file (except for console output)
        """
        try:
            if not scan_result.vulnerabilities and not scan_result.metrics:
                self.logger.warning("No scan results to generate report from")
                return "No results to report"

            output_formats = {
                "console": self._generate_console_report,
                "html": self._generate_html_report,
                "pdf": self._generate_pdf_report,
                "json": self._generate_json_report,
            }

            if output_format not in output_formats:
                raise SecurityScannerError(
                    f"Unsupported output format: {output_format}. "
                    f"Supported formats: {', '.join(output_formats.keys())}"
                )

            if not output_path:
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                if output_format == "console":
                    output_path = None
                else:
                    output_path = f"output/security_scan_report_{timestamp}.{output_format}"

            report_path = output_formats[output_format](scan_result, output_path)

            if output_path:
                self.logger.info(
                    f"{output_format.upper()} report generated: {report_path}"
                )

            return report_path

        except Exception as e:
            error_msg = f"Error generating {output_format} report: {str(e)}\n{traceback.format_exc()}"
            self.logger.error(error_msg)
            raise SecurityScannerError(error_msg) from e

    def _generate_json_report(
        self, scan_result: SecurityScanResult, output_path: Optional[str] = None
    ) -> str:
        """Generate a detailed JSON report of the security scan results.

        Args:
            scan_result: The complete scan result object
            output_path: Optional custom path for the JSON file

        Returns:
            str: Path to the generated JSON report
        """

        # Convert datetime objects to ISO format for JSON serialization
        def datetime_handler(obj: Any) -> str:
            if isinstance(obj, datetime):
                return obj.isoformat()
            if isinstance(obj, Path):
                return str(obj)
            if isinstance(obj, WindowsPath):
                return str(obj)
            raise TypeError(f"Object of type {type(obj)} is not JSON serializable")

        # Create a structured report dictionary
        report_data = {
            "scan_timestamp": datetime.now().strftime("%Y-%m-%d"),
            "summary": scan_result.scan_summary,
            "metrics": scan_result.metrics,
            "vulnerabilities": [
                {
                    "vulnerability_type": vuln.vulnerability_type,
                    "file_path": vuln.file_path,
                    "line_number": vuln.line_number,
                    "code_snippet": vuln.code_snippet,
                    "description": vuln.description,
                    "severity": vuln.severity,
                    "risk_score": vuln.risk_score,
                    "recommendation": vuln.recommendation,
                    "corrected_code": vuln.corrected_code,
                    "cwe_id": vuln.cwe_id,
                    "impact": vuln.impact,
                }
                for vuln in scan_result.vulnerabilities
            ],
            "risk_analysis": {
                "risk_distribution": self._calculate_risk_distribution(
                    scan_result.vulnerabilities
                ),
                "high_risk_files": self._identify_high_risk_files(
                    scan_result.vulnerabilities
                ),
                "vulnerability_trends": self._analyze_vulnerability_patterns(
                    scan_result.vulnerabilities
                ),
            },
        }

        # Ensure the output directory exists
        if output_path:
            output_dir = Path(output_path).parent
            output_dir.mkdir(parents=True, exist_ok=True)

        # Write the JSON report
        try:
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(
                    report_data,
                    f,
                    indent=2,
                    default=datetime_handler,
                    ensure_ascii=False,
                )
            return output_path
        except Exception as e:
            raise SecurityScannerError(f"Failed to generate JSON report: {str(e)}")

    def _calculate_risk_distribution(
        self, vulnerabilities: List[VulnerabilityFinding]
    ) -> Dict[str, Any]:
        """Calculate detailed risk distribution statistics."""
        return {
            "severity_counts": {
                "High": len([v for v in vulnerabilities if v.severity == "High"]),
                "Medium": len([v for v in vulnerabilities if v.severity == "Medium"]),
                "Low": len([v for v in vulnerabilities if v.severity == "Low"]),
            },
            "risk_score_ranges": {
                "critical": len([v for v in vulnerabilities if v.risk_score >= 9.0]),
                "high": len([v for v in vulnerabilities if 7.0 <= v.risk_score < 9.0]),
                "medium": len(
                    [v for v in vulnerabilities if 4.0 <= v.risk_score < 7.0]
                ),
                "low": len([v for v in vulnerabilities if v.risk_score < 4.0]),
            },
            "average_risk_score": (
                sum(v.risk_score for v in vulnerabilities) / len(vulnerabilities)
                if vulnerabilities
                else 0
            ),
        }

    def _identify_high_risk_files(
        self, vulnerabilities: List[VulnerabilityFinding]
    ) -> List[Dict[str, Any]]:
        """Identify files with high-risk vulnerabilities."""
        file_risks = {}
        for vuln in vulnerabilities:
            if vuln.file_path:
                if vuln.file_path not in file_risks:
                    file_risks[vuln.file_path] = {
                        "high_severity_count": 0,
                        "total_risk_score": 0,
                        "vulnerabilities": [],
                    }

                file_risks[vuln.file_path]["vulnerabilities"].append(
                    {
                        "type": vuln.vulnerability_type,
                        "severity": vuln.severity,
                        "risk_score": vuln.risk_score,
                    }
                )

                if vuln.severity == "High":
                    file_risks[vuln.file_path]["high_severity_count"] += 1
                file_risks[vuln.file_path]["total_risk_score"] += vuln.risk_score

        # Sort files by risk (high severity count and total risk score)
        high_risk_files = [
            {
                "file_path": file_path,
                "high_severity_count": stats["high_severity_count"],
                "average_risk_score": stats["total_risk_score"]
                / len(stats["vulnerabilities"]),
                "vulnerability_count": len(stats["vulnerabilities"]),
                "vulnerabilities": stats["vulnerabilities"],
            }
            for file_path, stats in file_risks.items()
        ]

        return sorted(
            high_risk_files,
            key=lambda x: (x["high_severity_count"], x["average_risk_score"]),
            reverse=True,
        )

    def _analyze_vulnerability_patterns(
        self, vulnerabilities: List[VulnerabilityFinding]
    ) -> Dict[str, Any]:
        """Analyze patterns in vulnerability occurrences."""
        return {
            "vulnerability_types": {
                v_type: len(
                    [v for v in vulnerabilities if v.vulnerability_type == v_type]
                )
                for v_type in set(v.vulnerability_type for v in vulnerabilities)
            },
            "common_cwe_ids": {
                cwe: len([v for v in vulnerabilities if v.cwe_id == cwe])
                for cwe in set(v.cwe_id for v in vulnerabilities if v.cwe_id)
            },
        }

    def _generate_console_report(self, scan_result: SecurityScanResult) -> None:
        """Generate rich console report"""
        console.print("\n[bold blue]Security Scan Report[/bold blue]")

        # Summary Table
        summary_table = Table(title="Scan Summary")
        summary_table.add_column("Metric", style="cyan")
        summary_table.add_column("Value", style="green")

        summary = scan_result.scan_summary
        summary_table.add_row(
            "Total Vulnerabilities", str(summary["total_vulnerabilities"])
        )
        summary_table.add_row("Files Scanned", str(summary["files_affected"]))
        summary_table.add_row("Lines Scanned", str(summary["total_lines_scanned"]))
        summary_table.add_row(
            "Average Risk Score", f"{summary['average_risk_score']:.2f}"
        )

        console.print(summary_table)

        # Vulnerabilities Table
        vuln_table = Table(title="\nDetailed Vulnerabilities")
        vuln_table.add_column("Type", style="red")
        vuln_table.add_column("File", style="blue")
        vuln_table.add_column("Line", style="cyan")
        vuln_table.add_column("Severity", style="yellow")
        vuln_table.add_column("Risk Score", style="magenta")

        for vuln in scan_result.vulnerabilities:
            vuln_table.add_row(
                vuln.vulnerability_type.value,
                vuln.file_path or "N/A",
                str(vuln.line_number),
                vuln.severity,
                f"{vuln.risk_score:.1f}",
            )

        console.print(vuln_table)

        # Detailed Findings
        console.print("\n[bold blue]Detailed Findings[/bold blue]")
        for i, vuln in enumerate(scan_result.vulnerabilities, 1):
            console.print(f"\n[bold red]Finding #{i}[/bold red]")
            console.print(f"[cyan]Type:[/cyan] {vuln.vulnerability_type.value}")
            console.print(
                f"[cyan]Location:[/cyan] {vuln.file_path or 'N/A'}:{vuln.line_number}"
            )
            console.print(
                f"[cyan]Vulnerable Code:[/cyan]\n```\n{vuln.code_snippet}\n```"
            )
            console.print(
                f"[cyan]Corrected Code:[/cyan]\n```\n{vuln.corrected_code}\n```"
            )
            console.print(f"[cyan]Description:[/cyan] {vuln.description}")
            console.print(f"[cyan]Impact:[/cyan] {vuln.impact}")
            console.print(f"[cyan]Recommendation:[/cyan] {vuln.recommendation}")
            if vuln.cwe_id:
                console.print(f"[cyan]CWE ID:[/cyan] {vuln.cwe_id}")

    def _generate_html_report(
        self,
        scan_result: SecurityScanResult,
        output_path: Optional[str] = "security_scan_report.html",
    ) -> str:
        """Generate an enhanced HTML report with interactive features and detailed styling."""
        template = Template(HTML_TEMPLATE)

        # Render the template with current timestamp
        html_content = template.render(
            summary=scan_result.scan_summary,
            vulnerabilities=scan_result.vulnerabilities,
            timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        )

        # Save HTML to file
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(html_content)

        return output_path

    def _generate_pdf_report(
        self,
        scan_result: SecurityScanResult,
        output_path: str = "security_scan_report.pdf",
    ) -> None:
        """Generate an enhanced PDF security report with better formatting and organization.

        Args:
            scan_result: SecurityScanResult object containing scan findings
            output_path: Path where PDF should be saved

        Returns:
            str: Path to generated PDF file
        """
        # Initialize document
        doc = SimpleDocTemplate(
            output_path,
            pagesize=letter,
            rightMargin=72,
            leftMargin=72,
            topMargin=72,
            bottomMargin=72,
        )

        # Dark background callback
        def set_dark_background(canvas, doc):
            canvas.setFillColor(colors.grey)  # Dark background color
            canvas.rect(0, 0, letter[0], letter[1], fill=1)  # Full-page rectangle

        # Initialize styles
        styles = getSampleStyleSheet()
        styles.add(
            ParagraphStyle(
                name="DarkBackground",
                parent=styles["Normal"],
                textColor=colors.white,
                backColor=colors.HexColor("#1A1A1A"),
                fontSize=12,
                spaceAfter=20,
                spaceBefore=20,
                padding=10,
            )
        )
        styles.add(
            ParagraphStyle(
                name="NormalWhite", parent=styles["Normal"], textColor=colors.white
            )
        )

        # Custom header style
        styles.add(
            ParagraphStyle(
                name="CustomHeader",
                parent=styles["Heading1"],
                fontSize=24,
                spaceAfter=30,
                textColor=colors.white,
                backColor=colors.darkblue,
            )
        )

        # Custom subheader style
        styles.add(
            ParagraphStyle(
                name="CustomSubHeader",
                parent=styles["Heading2"],
                fontSize=18,
                spaceAfter=20,
                textColor=colors.HexColor("#2C3E50"),
            )
        )

        # Initialize story (content elements)
        story = []

        # Title
        story.append(Paragraph("Security Scan Report", styles["CustomHeader"]))
        story.append(Spacer(1, 20))

        # Timestamp
        story.append(
            Paragraph(
                f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                styles["Normal"],
            )
        )
        story.append(Spacer(1, 20))

        # Executive Summary
        story.append(Paragraph("Executive Summary", styles["CustomSubHeader"]))
        summary_data = [
            [
                "Total Vulnerabilities",
                str(scan_result.scan_summary["total_vulnerabilities"]),
            ],
            ["Files Scanned", str(scan_result.scan_summary["files_affected"])],
            ["Lines Scanned", str(scan_result.scan_summary["total_lines_scanned"])],
            [
                "Average Risk Score",
                f"{scan_result.scan_summary['average_risk_score']:.2f}",
            ],
        ]

        summary_table = Table(
            summary_data,
            colWidths=[3 * inch, 2 * inch],
            style=TableStyle(
                [
                    ("BACKGROUND", (0, 0), (-1, -1), colors.lightgrey),
                    ("TEXTCOLOR", (0, 0), (-1, -1), colors.black),
                    ("ALIGN", (0, 0), (-1, -1), "LEFT"),
                    ("FONTNAME", (0, 0), (-1, -1), "Helvetica-Bold"),
                    ("FONTSIZE", (0, 0), (-1, -1), 12),
                    ("BOTTOMPADDING", (0, 0), (-1, -1), 12),
                    ("BACKGROUND", (0, 0), (-1, 0), colors.HexColor("#2C3E50")),
                    ("TEXTCOLOR", (0, 0), (-1, 0), colors.white),
                    ("GRID", (0, 0), (-1, -1), 1, colors.black),
                ]
            ),
        )
        story.append(summary_table)
        story.append(Spacer(1, 20))

        # Severity Distribution
        story.append(Paragraph("Severity Distribution", styles["CustomSubHeader"]))
        severity_data = [
            ["Severity", "Count"],
            ["High", str(scan_result.scan_summary["severity_distribution"]["High"])],
            [
                "Medium",
                str(scan_result.scan_summary["severity_distribution"]["Medium"]),
            ],
            ["Low", str(scan_result.scan_summary["severity_distribution"]["Low"])],
        ]

        severity_table = Table(
            severity_data,
            colWidths=[3 * inch, 2 * inch],
            style=TableStyle(
                [
                    ("ALIGN", (0, 0), (-1, -1), "LEFT"),
                    ("FONTNAME", (0, 0), (-1, 0), "Helvetica-Bold"),
                    ("FONTSIZE", (0, 0), (-1, -1), 12),
                    ("BOTTOMPADDING", (0, 0), (-1, -1), 12),
                    ("BACKGROUND", (0, 0), (-1, 0), colors.HexColor("#2C3E50")),
                    ("TEXTCOLOR", (0, 0), (-1, 0), colors.white),
                    ("BACKGROUND", (0, 1), (-1, 1), colors.red),
                    ("BACKGROUND", (0, 2), (-1, 2), colors.orange),
                    ("BACKGROUND", (0, 3), (-1, 3), colors.green),
                    ("TEXTCOLOR", (0, 1), (-1, 3), colors.white),
                    ("GRID", (0, 0), (-1, -1), 1, colors.black),
                ]
            ),
        )
        story.append(severity_table)
        story.append(PageBreak())

        # Detailed Findings
        story.append(Paragraph("Detailed Findings", styles["CustomSubHeader"]))
        for i, vuln in enumerate(scan_result.vulnerabilities, 1):
            # Vulnerability Header
            story.append(
                Paragraph(
                    f"Finding #{i}: {vuln.vulnerability_type}", styles["DarkBackground"]
                )
            )

            # Vulnerability Details Table
            details_data = [
                ["File", vuln.file_path or "N/A"],
                ["Line Number", str(vuln.line_number)],
                ["Severity", vuln.severity],
                ["Risk Score", f"{vuln.risk_score:.1f}"],
                ["CWE ID", vuln.cwe_id or "N/A"],
            ]

            details_table = Table(
                details_data,
                colWidths=[2 * inch, 4 * inch],
                style=TableStyle(
                    [
                        ("ALIGN", (0, 0), (-1, -1), "LEFT"),
                        ("FONTNAME", (0, 0), (-1, -1), "Helvetica"),
                        ("FONTSIZE", (0, 0), (-1, -1), 10),
                        ("BOTTOMPADDING", (0, 0), (-1, -1), 8),
                        ("BACKGROUND", (0, 0), (0, -1), colors.HexColor("#34495E")),
                        ("TEXTCOLOR", (0, 0), (0, -1), colors.white),
                        ("GRID", (0, 0), (-1, -1), 1, colors.black),
                    ]
                ),
            )
            story.append(details_table)
            story.append(Spacer(1, 10))

            # Description and Impact
            story.append(Paragraph("<b>Description:</b>", styles["Normal"]))
            story.append(Paragraph(vuln.description, styles["Normal"]))
            story.append(Spacer(1, 10))

            story.append(Paragraph("<b>Impact:</b>", styles["Normal"]))
            story.append(Paragraph(vuln.impact, styles["Normal"]))
            story.append(Spacer(1, 10))

            # Code Snippets
            story.append(Paragraph("<b>Vulnerable Code:</b>", styles["Normal"]))
            code_table = Table(
                [[Paragraph(vuln.code_snippet, styles["Code"])]],
                colWidths=[6 * inch],
                style=TableStyle(
                    [
                        ("BACKGROUND", (0, 0), (-1, -1), colors.HexColor("#F7F9F9")),
                        ("BOX", (0, 0), (-1, -1), 1, colors.black),
                        ("PADDING", (0, 0), (-1, -1), 8),
                    ]
                ),
            )
            story.append(code_table)
            story.append(Spacer(1, 10))

            story.append(Paragraph("<b>Corrected Code:</b>", styles["Normal"]))
            corrected_code_table = Table(
                [[Paragraph(vuln.corrected_code, styles["Code"])]],
                colWidths=[6 * inch],
                style=TableStyle(
                    [
                        ("BACKGROUND", (0, 0), (-1, -1), colors.HexColor("#E8F6F3")),
                        ("BOX", (0, 0), (-1, -1), 1, colors.black),
                        ("PADDING", (0, 0), (-1, -1), 8),
                    ]
                ),
            )
            story.append(corrected_code_table)
            story.append(Spacer(1, 10))

            # Recommendation
            story.append(Paragraph("<b>Recommendation:</b>", styles["Normal"]))
            story.append(Paragraph(vuln.recommendation, styles["Normal"]))
            story.append(PageBreak())

        # Build PDF
        doc.build(
            story, onFirstPage=set_dark_background, onLaterPages=set_dark_background
        )
        return output_path


def main():
    """Main function to test the security scanner"""
    code_to_scan = """
import sqlite3

def get_user(user_id):
    conn = sqlite3.connect('users.db')
    cursor = conn.cursor()
    
    # Vulnerable to SQL injection
    query = f"SELECT * FROM users WHERE id = '{user_id}'"
    cursor.execute(query)
    
    # Hardcoded credentials
    api_key = "1234567890abcdef"
    secret_token = "super_secret_123"
    
    return cursor.fetchone()
    """

    try:
        import path
        import sys

        # directory reach
        directory = path.Path(__file__).absolute()

        # setting path
        sys.path.append(directory.parent.parent)
        from code_assistant import CodeAssistant

        code_assistant = CodeAssistant()

        scanner = SecurityScanner(code_assistant)

        # Scan a directory of Python files
        scan_result = scanner.scan_files("./test_scanner")
        scanner.generate_report(scan_result, "json")

        # Or scan a single piece of code
        # direct_scan_result = scanner.scan_code(code_to_scan)
        # scanner.generate_report(direct_scan_result, "pdf")

    except Exception as e:
        logger.error(f"Error in main: {str(e)}")
        raise


if __name__ == "__main__":
    main()
